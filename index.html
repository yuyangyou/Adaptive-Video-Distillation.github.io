<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Adaptive-Video-Distillation | CVPR 2026</title>

<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">

<style>
:root {
  --accent: #6C63FF;
  --text: #111;
  --bg: #fff;
  --card: #f7f7f9;
}

@media (prefers-color-scheme: dark) {
  :root {
    --text: #eee;
    --bg: #111;
    --card: #1c1c1f;
  }
}

body {
  margin: 0;
  font-family: 'Inter', sans-serif;
  background: var(--bg);
  color: var(--text);
  line-height: 1.7;
}

.container {
  max-width: 1000px;
  margin: auto;
  padding: 60px 20px;
}

h1 {
  font-size: 48px;
  font-weight: 800;
  text-align: center;
  background: linear-gradient(90deg, #6C63FF, #00BFA6);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.subtitle {
  text-align: center;
  font-size: 20px;
  margin-top: 10px;
}

.authors {
  text-align: center;
  margin-top: 15px;
  font-size: 18px;
}

.links {
  text-align: center;
  margin-top: 25px;
}

.links a {
  display: inline-block;
  margin: 8px;
  padding: 10px 18px;
  border-radius: 8px;
  background: var(--card);
  text-decoration: none;
  color: var(--text);
  font-weight: 600;
  transition: 0.2s;
}

.links a:hover {
  background: var(--accent);
  color: white;
}

.section {
  margin-top: 100px;
}

.section h2 {
  font-size: 32px;
  margin-bottom: 20px;
}

.authors sup {
  font-size: 12px;
}
  
.card {
  background: var(--card);
  padding: 30px;
  border-radius: 14px;
}

img {
  width: 100%;
  border-radius: 12px;
  margin-top: 20px;
}

video {
  width: 100%;
  border-radius: 12px;
  margin-top: 20px;
}

ul li {
  margin-bottom: 12px;
}

.fade-in {
  opacity: 0;
  transform: translateY(20px);
  transition: 1s ease;
}

.fade-in.visible {
  opacity: 1;
  transform: translateY(0);
}

footer {
  text-align: center;
  margin-top: 120px;
  opacity: 0.6;
}
</style>

<script>
window.addEventListener("scroll", function() {
  document.querySelectorAll(".fade-in").forEach(function(el) {
    const rect = el.getBoundingClientRect();
    if (rect.top < window.innerHeight - 100) {
      el.classList.add("visible");
    }
  });
});
</script>

<!-- MathJax -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<div class="container">

<h1>Adaptive Video Distillation: Mitigating Oversaturation and Temporal Collapse in Few-Step Generation</h1>

<div class="authors">
Yuyang You<sup>1</sup>,
Yongzhi Li<sup>2</sup>,
Jiahui Li<sup>2</sup>,
Quan Chen<sup>2</sup>,
Peng Jiang<sup>2</sup>,
Yadong Mu<sup>1†</sup>
<br><br>

<sup>1</sup> Peking University &nbsp;&nbsp;
<sup>2</sup> kuaishou Technology &nbsp;&nbsp;
<br>
<sup>†</sup> Corresponding Author
<br><br>

<b>CVPR 2026</b>
</div>

<div class="links">
<a href="#">Paper</a>
<a href="#">Code</a>
</div>

<div class="section fade-in">
<h2>Overview</h2>
<div class="card">
<p>
We incorporate Adaptive Regression Loss and Temporal Regularization Loss into Distribution Matching Distillation (DMD) to mitigate oversaturation and low dynamism in video tasks. Furthermore, our approach enables Supervised Fine-tuning (SFT) concurrently with distillation, facilitating effective style transfer.
</p>
</div>
</div>

<div class="section fade-in">
<h2>Video Results</h2>
<div class="card">

<video controls autoplay muted loop playsinline>
  <source src="demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>
Comparison between baseline DMD and our Adaptive Video Distillation.
</p>

</div>
</div>

<div class="section fade-in">
<h2>Method</h2>
<div class="card">
<p>
Our method distills a pre-trained teacher model, denoted as $s_{\text{data}}$, into a few-step video generator $G_\phi$. The training procedure consists of the following steps:(1) A batch of real video-text pairs is sampled from the dataset. After applying noise perturbations to the videos, the student model performs denoising reconstruction. A regression loss is computed between the reconstructed video and the ground-truth video. Subsequently, this loss is adaptively weighted using our Loss Mean Cache to produce the final adaptive regression loss (see Sec.~\ref{subsec:temp} for details). (2) Text conditions are sampled from the dataset to guide the student model in generating a video from pure noise. The denoised output from this process is used to compute a temporal regularization loss (Eq.~\ref{eq:temp}) and a distribution matching loss (Eq.~\ref{eq:dmd}).(3) Finally, the generator $G_\phi$ is updated via gradient descent using the combined losses. The $s_{\text{gen}, \xi}$ in DMD are updated separately, following the methodology of DMD2 (this particular update step is not depicted in the figure for clarity).
</p>
<img src="main.png">
</div>
</div>

<div class="section fade-in">
<h2>Results</h2>
<div class="card">
<img src="results.png">
<p>
Our method consistently outperforms prior works.
</p>
</div>
</div>

<div class="section fade-in">
<h2>BibTeX</h2>
<div class="card">
<pre>
@inproceedings{yourname2026paper,
  title={Your Paper Title},
  author={Author1 and Author2 and Author3},
  booktitle={CVPR},
  year={2026}
}
</pre>
</div>
</div>

<footer>
Designed for CVPR 2026
</footer>

</div>
</body>
</html>
